<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Portfolio</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav id="navbar">
        <ul>
            <li><a href="#about">About</a></li>
            <li><a href="#projects">Projects</a></li>
            <li><a href="#experiences">Experiences</a></li>
            <li><a href="#skills">Skills</a></li>
            <li><a href="#courses">Courses</a></li>
        </ul>
    </nav>

    <section id="about">
        <!-- Left Section (Photo and Subheading) -->
        <div class="left-section">
            <div class="photo-container">
                <img src="./images/photo.jpg" alt="Photograph" class="profile-photo">
            </div>
            <h2 class="name">Jitesh Chavan (jsc78@njit.edu)</h2>
            <h3 class="subheading">Generative AI | Computer Vision | NLP</h3>
            <!-- Icon Container -->
        <div class="icon-container">
            <a href="https://github.com/JiteshChavan" target="_blank" class="icon-link">
                <img src="./images/GitHub.png" alt="GitHub" class="icon">
            </a>
            <a href="mailto:jchavan010@gmail.com" class="icon-link">
                <img src="./images/mail.png" alt="Email" class="icon">
            </a>
            <a href="https://www.linkedin.com/in/jchavan24/" target="_blank" class="icon-link">
                <img src="./images/LinkedIn.png" alt="LinkedIn" class="icon">
            </a>
        </div>
        </div>
    
        <!-- Right Section (Glossy Rectangle) -->
        <div class="glossy-rectangle">
            <h2>About Me</h2>
            <p>I am a Computer Science graduate student at the New Jersey Institute of Technology.
                My research focuses Machine Learning Algorithms, in particular Generative Modeling and connections to Mathematics, and Engineering and Scaling Novel Foundation Models with an emphasis on their application in the domain of Large Language Models to model Cross-Modal and Multi-Modal generative tasks.
            </p>
            <p>
                I have experience scaling Autoregressive Generative Models for Natural Language Processing, Score-Based and Flow Matching (denoising score matching and diffusion) models for Cross-Modal generative modeling, Variational Autoencoders and Generative Adversarial Networks.
            </p>
            
            <p>
               I am currently preparing a research draft for the <a href="https://aaai.org/conference/aaai/aaai-26/main-technical-track-call/" target="_blank" rel="noopener noreferrer">AAAI 2026 conference</a>, focused on eliminating causal and directional bias in state space (Mamba) models by proposing architectural modifications
                to generalize them for generative modeling in non-sequential modalities such as 3D Gaussian Splatting and image generation.
            </p>
            
            <p>
                I recently Engineered a 1.2B parameter Elucidated Latent Diffusion Sparse Transformer Model for text guided image generation achieving state of the art results at a micro-budget.
                You can find more details about the project below.
            <p>
                Prior to this I worked at Ford Motor Company as a Software Developer, where I was responsible for implementation BERT variants of NLP models to help evaluate the recommendation system and optimizing REST APIs for Ford's ANZ region e-commerce platform.
            </p>

            <p>
                Besides my work, I have passion for playing football, guitar and hiking.
            </p>
            <p>
                If you'd like to take a look at my CV, drop me an email : jchavan010@gmail.com
            </p>
        </div>
    </section>

    <section id="projects">
        <h2>Projects</h2>
        <div class="projects-container">
            <!-- Project 1 -->
            <div class="project-item">
                <div class="project-images">
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Micro-Budget-Sparse-Elucidated-Latent-Diffusion-Transformer', '_blank')">
                        <img src="./images/image_grid.png" alt="Project 1">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Micro-Budget-Sparse-Elucidated-Latent-Diffusion-Transformer', '_blank')">
                        <img src="./images/image_grid_2.png" alt="Project 1 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/Micro-Budget-Sparse-Elucidated-Latent-Diffusion-Transformer" target="_blank">Elucidated Latent Diffusion Sparse Transformer Model</a></h3>
                    <p>
                        Engineered a 1.2B parameter Elucidated Latent Diffusion Sparse Transformer Model for text-guided image generation, achieving state-of-the-art results on a micro-budget.
                        This project involved state of the art Cost Optimization techniques like patch masking to reduce input sequence length to the transformer backbone,
                        lightweight patchmixer backbone prior to patch masking and the cardinal DiT backbone to alleviate the downsides of patch masking.
                        Finally progressive Pretraining-Finetuning pipeline, with 75% patch masking and no patch masking respectively, enabling effective masked and unmasked denoising representation learning.
                    </p>
                    <p><strong>Skills:</strong>Multi-Modal / Cross-Modal Generative Modeling, Score Based Generative (Diffusion) Models, Computer Vision, Natural Language Processing</p>
                    <p>The panels on the left depict inferences using the trained model for various prompts.
                        Click on the panels on the left to head over to the repository for more details.
                    </p>
                </div>
            </div>
    
            <!-- Project 2 -->
            <div class="project-item">
                <div class="project-images">
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Transformer-based-autoregressive-image-generation-using-VQGAN', '_blank')">
                        <img src="./images/blind_render.png" alt="Project 2">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Transformer-based-autoregressive-image-generation-using-VQGAN', '_blank')">
                        <img src="./images/half_ctx.png" alt="Project 2 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/Transformer-based-autoregressive-image-generation-using-VQGAN" target="_blank">Transformer-Based Autoregressive Image Generation using VQGAN</a></h3>
                    <p>
                        Designed a 199.5M parameter generative model for transformer-based autoregressive image synthesis, that utilizes a variant of GPT-2 architecture (by OpenAI) to auto-regressively generate images
                        in the latent space of a Discrete Neural Representation Learning model consisting of Vector Quantized Variational Autoencoder trained with adversarial setup (VQGAN). 
                        The VQGAN model, was optimized using a multi-loss objective, including pixel-space reconstruction loss, perceptual loss (via pretrained LPIPS), adversarial loss from a patch discriminator, 
                        and quantization loss for a highly entangled latent space.
                    </p>
                    <p><strong>Skills:</strong>Generative AI, Computer Vision, Deep Learning, Convolutional Neural Networks (CNN), Transformers, Autoregressive Models, Variational Autoencoders</p>
                    <p>
                        The first panels on the left depict blind context renders (first panel) and half context image completions respectively.
                        Click on the panels on the left to head over to the repository for more information.
                    </p>
                </div>
            </div>

            <!-- Project 3 -->
            <div class="project-item">
                <div class="project-images project-3-images"> <!-- Added unique class -->
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/nanogpt', '_blank')">
                        <img src="./images/gpt_inference_1.png" alt="Project 2">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/nanogpt', '_blank')">
                        <img src="./images/gpt_inference_2.png" alt="Project 2 Additional Image">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/nanogpt', '_blank')">
                        <img src="./images/gpt_inference_3.png" alt="Project 2 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/nanogpt" target="_blank">
                        Generative AI : Foundation Model for Text Completion</a></h3>
                    <p>
                        Designed a 124M parameter autoregressive language model from scratch, based on OpenAI's GPT-2 architecture, with a context length of 1024 tokens.
                        This project involed training the said model on 8x A100 SXM GPU cluster using PyTorch's Distributed Data Parallel, BFloat16 mixed precision, flash attention, and kernel fusion.
                        When trained on 50B GPT-2 tokens from FineWebEdu dataset, the model matches GPT-3 124M model's HellaSwag evaluation score of 33.70%
                    </p>
                    <p><strong>Skills:</strong>Natural Language Processing (NLP), Large Language Models (LLM), Generative AI, Deep Learning</p>
                    <p>Click on the panels on the left to head over to the repository</p>
                </div>
            </div>

            <div class="project-item single-panel-project">
                <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/U-Net-AutoEncoder-Variant-with-self-attention-block-in-bottleneck', '_blank')">
                    <img src="./images/unet.png" alt="U-Net AutoEncoder">
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/nanogpt" target="_blank">
                        U-net based Neural Representation Learning Autoencoder Model</a></h3>
                    <p>
                        Implemented a U-Net based Neural Representation Learning Model from scratch in PyTorch with fully Convolutional encoder and Decoder with Self Attention Block to refine global context in the U-Net bottleneck.
                    </p>
                    <p><strong>Skills:</strong> Deep Learning, Computer Vision, Autoencoder Networks, Convolutional Neural Networks</p>
                    <p>The panel on the left illustrates reconstruction inferences using the trained U-Net.</p>
                </div>
            </div>

            <div class="project-item">
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/Micrograd" target="_blank">MicrogradEngine</a></h3>
                    <p>
                        An atomic Autogradient engine. Implements backpropagation (reverse-mode autodifferentiation) 
                        over a dynamically built Directed Acyclic
                        Graph (DAG) andasmallneuralnetworkslibrary on top of it with a PyTorch-like API.
                    </p>
                    <p><strong>Skills:</strong> Pytorch, Python</p>
                </div>
            </div>
            <!-- Add more projects as needed -->
        </div>
    </section>

    <section id="experiences">
        <h2>Experiences</h2>
        <div class="experience-item">
            <p class="company">Graduate Student Researcher</p> <!-- Company name moved here -->
            <h3>Guttenberg Information Technologies Center (GITC), NJIT </h3> <!-- Role moved here -->
            <p class="location-duration">Newark, NJ, USA | Feb 2024 - May 2025</p>
            <ul class="responsibilities">
                Implemented cross-modal generative models including Flow Matching, diffusion, VAEs, GANs, and SDE‑based models for text guided Image
                Generation.
                Prototyped alternatives to Transformer backbones using state space models (Mamba/S6) with spatial‑frequency domain transformations.
                Built a data processing pipeline to support research in LLMs for symbolic mathematics and calculus problem solving.
            </ul>
        </div>
        <div class="experience-item">
            <p class="company">Software Engineer - Machine Learning Team</p> <!-- Company name moved here -->
            <h3>Ford Motor Company</h3> <!-- Role moved here -->
            <p class="location-duration">Chennai, Tamil Nadu, India | July 2021 - Dec 2022</p>
            <ul class="responsibilities">
                At Ford Motor Company, I optimized REST API latency by 58% on the APAC e-commerce platform through cross-functional collaboration with ML and backend teams.
                I developed a BERT-based NLP evaluation pipeline to enhance recommendation quality, contributing to a 22% increase in transactions and 38.6% traffic growth.
                Additionally, I built scalable backend systems in Spring Boot/MVC and prototyped ML-driven content management workflows within the SAP Hybris framework as part of Ford’s “Shop–Buy–Own” digital initiative.
            </ul>
        </div>
        <div class="experience-item">
            <p class="company">Research Assistant, Computer Vision Lab</p> <!-- Company name moved here -->
            <h3>Visvesvaraya National Institute of Technology</h3> <!-- Role moved here -->
            <p class="location-duration">Nagpur, Maharashtra, India | July 2020 - May 2021</p>
            <ul class="responsibilities">
                Researched real-time object detection and recognition systems on memory-constrained single-board computers,
                Which helped optimize the <strong>Haar Cascade Classifier</strong> in C for single-board computer constraints, improving efficiency in resource-limited environments for the Final Year Undergrad Project : Real Time Face detection
                and recognition using Multi-Tasked Cascaded Convolutional Neural Network.
                Also researched synthetic dataset generation using <strong>Generative Adversarial Networks (GANs)</strong> for computer vision applications.
            </ul>
        </div>
    </section>
    

    <section id="skills">
        <h2>Skills</h2>
        <div class="skills-container">
            <!-- Skills will be dynamically populated here -->
        </div>
    </section>

    <section id="courses">
        <h2>Courses</h2>
        <div class="flair-line"></div> <!-- Flair line below the heading -->
        <div class="courses-container">
            <div class="course-card">
                Introduction to Deep Learning | MIT 6.S191
                <a href="https://introtodeeplearning.com/" target="_blank" class="course-link">MIT OCW</a>
            </div>
            <div class="course-card">
                TinyML and Efficient Deep Learning Computing | MIT 6.5940
                <a href="https://hanlab.mit.edu/courses/2023-fall-65940" target="_blank" class="course-link">MIT OCW</a>
            </div>
            <div class="course-card">Computer Vision | ECL423
                <a href="https://vnit.ac.in/section/academics/wp-content/uploads/2023/07/B.Tech-in-Electronic-and-Communication-Enggineering.pdf" target="_blank" class="course-link">VNIT core coursework</a>
            </div>
            <div class="course-card">Multivariate Calculus, Integral Transforms and Partial Differential Equations | MAL201
                <a href="https://vnit.ac.in/section/academics/wp-content/uploads/2023/07/B.Tech-in-Electronic-and-Communication-Enggineering.pdf" target="_blank" class="course-link">VNIT core coursework</a>
            </div>
            <div class="course-card">Statistical Analysis and Queing Theory | MAL408
                <a href="https://vnit.ac.in/section/academics/wp-content/uploads/2023/07/B.Tech-in-Electronic-and-Communication-Enggineering.pdf" target="_blank" class="course-link">VNIT core coursework</a>
            </div>

            <div class="course-card">Machine Learning | CS675
                <a href="https://catalog.njit.edu/graduate/computing-sciences/computer-science/ms/" target="_blank" class="course-link">NJIT core coursework</a>
            </div>

            <div class="course-card">Artificial Intelligence | CS670
                <a href="https://catalog.njit.edu/graduate/computing-sciences/computer-science/ms/" target="_blank" class="course-link">NJIT core coursework</a>
            </div>

        </div>
    </section>
    <script src="script.js"></script>
</body>
</html>