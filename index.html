<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Portfolio</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav id="navbar">
        <ul>
            <li><a href="#about">About</a></li>
            <li><a href="#projects">Projects</a></li>
            <li><a href="#experiences">Experiences</a></li>
            <li><a href="#skills">Skills</a></li>
            <li><a href="#courses">Courses</a></li>
        </ul>
    </nav>

    <section id="about">
        <!-- Left Section (Photo and Subheading) -->
        <div class="left-section">
            <div class="photo-container">
                <img src="./images/photo.jpg" alt="Photograph" class="profile-photo">
            </div>
            <h2 class="name">Jitesh Chavan</h2>
            <h3 class="subheading">Generative AI | Computer Vision | NLP</h3>
            <!-- Icon Container -->
        <div class="icon-container">
            <a href="https://github.com/JiteshChavan" target="_blank" class="icon-link">
                <img src="./images/GitHub.png" alt="GitHub" class="icon">
            </a>
            <a href="mailto:jchavan010@gmail.com" class="icon-link">
                <img src="./images/mail.png" alt="Email" class="icon">
            </a>
            <a href="https://www.linkedin.com/in/jchavan24/" target="_blank" class="icon-link">
                <img src="./images/LinkedIn.png" alt="LinkedIn" class="icon">
            </a>
        </div>
        </div>
    
        <!-- Right Section (Glossy Rectangle) -->
        <div class="glossy-rectangle">
            <h2>About Me</h2>
            <p>I am a passionate Computer Science graduate student at the New Jersey Institute of Technology, deeply invested in building and scaling Generative AI models.
                My current research focuses Score-Based Generative Models and Diffusion Models, with an emphasis on their application in the domain of Large Language Models to model Cross-Modal and Multi-Modal generative tasks.
            </p>
            <p>
                I recently Engineered a 1.2B parameter Elucidated Latent Diffusion Sparse Transformer Model for text guided image generation achieving state of the art results at a micro-budget.
                You can find more details about the project below.
            <p>
                I have experience working with Score-based Generative Models, Denoising Score Matching Models, Variational Autoencoders, Autoregressive Models and Generative Adversarial Networks. 
            </p>
            <p>
                Outside of my research, I like to play guitar, football (the real kind) and baking.
            </p>
            <p>
                If you'd like to take a look at my CV, drop me an email : jchavan010@gmail.com
            </p>
        </div>
    </section>

    <section id="projects">
        <h2>Projects</h2>
        <div class="projects-container">
            <!-- Project 1 -->
            <div class="project-item">
                <div class="project-images">
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Micro-Budget-Sparse-Elucidated-Latent-Diffusion-Transformer', '_blank')">
                        <img src="./images/image_grid.png" alt="Project 1">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Micro-Budget-Sparse-Elucidated-Latent-Diffusion-Transformer', '_blank')">
                        <img src="./images/image_grid_2.png" alt="Project 1 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/Micro-Budget-Sparse-Elucidated-Latent-Diffusion-Transformer" target="_blank">Elucidated Latent Diffusion Sparse Transformer Model</a></h3>
                    <p>
                        Engineered a 1.2B parameter Elucidated Latent Diffusion Sparse Transformer Model for text-guided image generation, achieving state-of-the-art results on a micro-budget.
                        This project involved state of the art Cost Optimization techniques like patch masking to reduce input sequence length to the transformer backbone,
                        lightweight patchmixer backbone prior to patch masking and the cardinal DiT backbone to alleviate the downsides of patch masking.
                        Finally progressive Pretraining-Finetuning pipeline, with 75% patch masking and no patch masking respectively, enabling effective masked and unmasked denoising representation learning.
                    </p>
                    <p><strong>Skills:</strong>Multi-Modal / Cross-Modal Generative Modeling, Score Based Generative (Diffusion) Models, Computer Vision, Natural Language Processing</p>
                    <p>Click on the panels on the left to head over to the repository</p>
                </div>
            </div>
    
            <!-- Project 2 -->
            <div class="project-item">
                <div class="project-images">
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Transformer-based-autoregressive-image-generation-using-VQGAN', '_blank')">
                        <img src="./images/blind_render.png" alt="Project 2">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Transformer-based-autoregressive-image-generation-using-VQGAN', '_blank')">
                        <img src="./images/half_ctx.png" alt="Project 2 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/Transformer-based-autoregressive-image-generation-using-VQGAN" target="_blank">Transformer-Based Autoregressive Image Generation using VQGAN</a></h3>
                    <p>
                        Designed a 199.5M parameter generative model for transformer-based autoregressive image synthesis, that utilizes a variant of GPT-2 architecture (by OpenAI) to auto-regressively generate images
                        in the latent space of a Discrete Neural Representation Learning model consisting of Vector Quantized Variational Autoencoder trained with adversarial setup (VQGAN). 
                        The VQGAN model, was optimized using a multi-loss objective, including pixel-space reconstruction loss, perceptual loss (via pretrained LPIPS), adversarial loss from a patch discriminator, 
                        and quantization loss for a highly entangled latent space.
                    </p>
                    <p><strong>Skills:</strong>Generative AI, Computer Vision, Deep Learning, Convolutional Neural Networks (CNN), Transformers, Autoregressive Models, Variational Autoencoders</p>
                    <p>Click on the panels on the left to head over to the repository</p>
                </div>
            </div>

            <!-- Project 3 -->
            <div class="project-item">
                <div class="project-images project-3-images"> <!-- Added unique class -->
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/nanogpt', '_blank')">
                        <img src="./images/gpt_inference_1.png" alt="Project 2">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/nanogpt', '_blank')">
                        <img src="./images/gpt_inference_2.png" alt="Project 2 Additional Image">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/nanogpt', '_blank')">
                        <img src="./images/gpt_inference_3.png" alt="Project 2 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/nanogpt" target="_blank">
                        Generative AI : Foundation Model for Text Completion</a></h3>
                    <p>
                        Designed a 124M parameter autoregressive language model from scratch, based on OpenAI's GPT-2 architecture, with a context length of 1024 tokens.
                        This project involed training the said model on 8x A100 SXM GPU cluster using PyTorch's Distributed Data Parallel, BFloat16 mixed precision, flash attention, and kernel fusion.
                        When trained on 50B GPT-2 tokens from FineWebEdu dataset, the model matches GPT-3 124M model's HellaSwag evaluation score of 33.70%
                    </p>
                    <p><strong>Skills:</strong>Natural Language Processing (NLP), Large Language Models (LLM), Generative AI, Deep Learning</p>
                    <p>Click on the panels on the left to head over to the repository</p>
                </div>
            </div>

            <div class="project-item">
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/Micrograd" target="_blank">MicrogradEngine</a></h3>
                    <p>
                        An atomic Autogradient engine. Implements backpropagation (reverse-mode autodifferentiation) 
                        over a dynamically built Directed Acyclic
                        Graph (DAG) andasmallneuralnetworkslibrary on top of it with a PyTorch-like API.
                    </p>
                    <p><strong>Skills:</strong> Pytorch, Python</p>
                </div>
            </div>
            <!-- Add more projects as needed -->
        </div>
    </section>

    <section id="experiences">
        <h2>Experiences</h2>
        <div class="experience-item">
            <p class="company">Ford Motor Company</p> <!-- Company name moved here -->
            <h3>Software Engineer</h3> <!-- Role moved here -->
            <p class="location-duration">Chennai, Tamil Nadu, India | July 2021 - Dec 2022</p>
            <ul class="responsibilities">
                Contributed to pptimizing REST APIs for the APAC region (ANZ, India, South Africa) e-commerce project,
                and enhancing Ford’s e-commerce platform using <strong>Spring MVC/Boot</strong> frameworks.
                Also developed content management prototypes for Ford’s <strong>‘Shop-Buy-Own Customer Journey’</strong> project using <strong>SAP Hybris</strong> framework.
            </ul>
        </div>
        <div class="experience-item">
            <p class="company">Visvesvaraya National Institute of Technology</p> <!-- Company name moved here -->
            <h3>Research Assistant, Computer Vision Lab</h3> <!-- Role moved here -->
            <p class="location-duration">Nagpur, Maharashtra, India | July 2020 - May 2021</p>
            <ul class="responsibilities">
                Researched real-time object detection and recognition systems on memory-constrained single-board computers,
                Which helped optimize the <strong>Haar Cascade Classifier</strong> in C for single-board computer constraints, improving efficiency in resource-limited environments for the Final Year Undergrad Project : Real Time Face detection
                and recognition using Multi-Tasked Cascaded Convolutional Neural Network.
                Also researched synthetic dataset generation using <strong>Generative Adversarial Networks (GANs)</strong> for computer vision applications.
            </ul>
        </div>
    </section>
    

    <section id="skills">
        <h2>Skills</h2>
        <div class="skills-container">
            <!-- Skills will be dynamically populated here -->
        </div>
    </section>

    <section id="courses">
        <h2>Courses</h2>
        <p>This section is about my courses.</p>
    </section>

    <script src="script.js"></script>
</body>
</html>