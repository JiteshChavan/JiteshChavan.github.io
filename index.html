<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Portfolio</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav id="navbar">
        <ul>
            <li><a href="#about">About</a></li>
            <li><a href="#publications">publications</a></li>
            <li><a href="#projects">Projects</a></li>
            <li><a href="#Articles">Articles</a></li>
            <li><a href="#experience">Experience</a></li>
            <li><a href="#skills">Skills</a></li>
            <li><a href="#courses">Courses</a></li>
        </ul>
    </nav>

    <section id="about">
        <!-- Left Section (Photo and Subheading) -->
        <div class="left-section">
            <div class="photo-container">
                <img src="./images/photo.jpg" alt="Photograph" class="profile-photo">
            </div>
            <h2 class="name">Jitesh Chavan</h2>
            <h3 class="subheading">Generative AI | Computer Vision | NLP</h3>
            <!-- Icon Container -->
        <div class="icon-container">
            <a href="https://github.com/JiteshChavan" target="_blank" class="icon-link">
                <img src="./images/GitHub.png" alt="GitHub" class="icon">
            </a>
            <a href="mailto:jchavan010@gmail.com" class="icon-link">
                <img src="./images/mail.png" alt="Email" class="icon">
            </a>
            <a href="https://www.linkedin.com/in/jchavan24/" target="_blank" class="icon-link">
                <img src="./images/LinkedIn.png" alt="LinkedIn" class="icon">
            </a>
        </div>
        </div>
    
        <!-- Right Section (Glossy Rectangle) -->
        <div class="glossy-rectangle">
            <h2>About Me</h2>
            <p>I am a Computer Science Graduate student at the NJIT. My research focuses Machine Learning Algorithms, in particular Generative Modeling and connections to Mathematics and Physics. I work on engineering and scaling Foundation Models with an emphasis on their applications for generative tasks in non sequential modalities (images, protien structures) and for Large Language Models.   
            </p>
            <p>
                As a Graduate Student Researcher at NJIT, I have extensively worked on Physics based Generative Foundation Models including Flow Matching, Score-Matching, Denoising Diffusion Models for downstream generative tasks such as text/stroke guided image generation and image inpainting via stochastic interpolants & control net frameworks.
                Before NJIT, I earned a B.Tech. in Electronics at VNIT, where I worked with <a href="https://dblp.org/pid/34/7796.html">K. M. Bhurchandi</a> on Computer Vision models for memory constrained single board computers, adversarial training and Variational Auto Encoders.
            </p> 
            <p>
               I am currently preparing a research draft on eliminating causal inductive bias in state space (Mamba) models by proposing architectural modifications
                to generalize them for generative modeling in non-sequential modalities such as 3D Gaussian Splatting and image generation.
            </p>           
            <p>
                Prior to this I worked at Ford Motor Company as a Software Engineer on a Machine Learning Team, where I was responsible for instrumentation of transformer encoder backbones and BERT based NLP models for the e-commerce platform recommendation system and its evaluation pipeline.
            </p>

            <p>
                Besides my work, I have passion for playing football, guitar and hiking.
            </p>
            <p>
                If you'd like to take a look at my CV, drop me an email : jchavan010@gmail.com
            </p>
        </div>
    </section>

    <section id="projects">
        <h2>Projects</h2>
        <div class="projects-container">
            <!-- Project 1 -->
            <div class="project-item">
                <div class="project-images">
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Micro-Budget-Sparse-Elucidated-Latent-Diffusion-Transformer', '_blank')">
                        <img src="./images/grid.gif" alt="Project 1">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Micro-Budget-Sparse-Elucidated-Latent-Diffusion-Transformer', '_blank')">
                        <img src="./images/image_grid.png" alt="Project 1 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/Micro-Budget-Sparse-Elucidated-Latent-Diffusion-Transformer" target="_blank">Elucidated Latent Diffusion Sparse Transformer Model</a></h3>
                    <p>
                        Engineered a 1.2B parameter Elucidated Latent Diffusion Sparse Transformer Model for text-guided image generation, achieving state-of-the-art results on a micro-budget.
                        This project involved state of the art Cost Optimization techniques like patch masking to reduce input sequence length to the transformer backbone,
                        lightweight patchmixer backbone prior to patch masking and the cardinal DiT backbone to alleviate the downsides of patch masking.
                        Finally progressive Pretraining-Finetuning pipeline, with 75% patch masking and no patch masking respectively, enabling effective masked and unmasked denoising representation learning.
                    </p>
                    <p><strong>Skills:</strong>Multi-Modal / Cross-Modal Generative Modeling, Score Based Generative (Diffusion) Models, Computer Vision, Natural Language Processing</p>
                    <p>The panels on the left depict inferences using the trained model for various prompts.
                        Click on the panels on the left to head over to the repository for more details.
                    </p>
                </div>
            </div>
    
            <!-- Project 2 -->
            <div class="project-item">
                <div class="project-images">
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Transformer-based-autoregressive-image-generation-using-VQGAN', '_blank')">
                        <img src="./images/blind_render.png" alt="Project 2">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/Transformer-based-autoregressive-image-generation-using-VQGAN', '_blank')">
                        <img src="./images/half_ctx.png" alt="Project 2 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/Transformer-based-autoregressive-image-generation-using-VQGAN" target="_blank">Transformer-Based Autoregressive Image Generation using VQGAN</a></h3>
                    <p>
                        Designed a 199.5M parameter generative model for transformer-based autoregressive image synthesis, that utilizes a variant of GPT-2 architecture (by OpenAI) to auto-regressively generate images
                        in the latent space of a Discrete Neural Representation Learning model consisting of Vector Quantized Variational Autoencoder trained with adversarial setup (VQGAN). 
                        The VQGAN model, was optimized using a multi-loss objective, including pixel-space reconstruction loss, perceptual loss (via pretrained LPIPS), adversarial loss from a patch discriminator, 
                        and quantization loss for a highly entangled latent space.
                    </p>
                    <p><strong>Skills:</strong>Generative AI, Computer Vision, Deep Learning, Convolutional Neural Networks (CNN), Transformers, Autoregressive Models, Variational Autoencoders</p>
                    <p>
                        The first panels on the left depict blind context renders (first panel) and half context image completions respectively.
                        Click on the panels on the left to head over to the repository for more information.
                    </p>
                </div>
            </div>

            <!-- Project 3 -->
            <div class="project-item">
                <div class="project-images project-3-images"> <!-- Added unique class -->
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/nanogpt', '_blank')">
                        <img src="./images/gpt_inference_1.png" alt="Project 2">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/nanogpt', '_blank')">
                        <img src="./images/gpt_inference_2.png" alt="Project 2 Additional Image">
                    </div>
                    <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/nanogpt', '_blank')">
                        <img src="./images/gpt_inference_3.png" alt="Project 2 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/nanogpt" target="_blank">
                        Generative AI : Foundation Model for Text Completion</a></h3>
                    <p>
                        Designed a 124M parameter autoregressive language model from scratch, based on OpenAI's GPT-2 architecture, with a context length of 1024 tokens.
                        This project involed training the said model on 8x A100 SXM GPU cluster using PyTorch's Distributed Data Parallel, BFloat16 mixed precision, flash attention, and kernel fusion.
                        When trained on 50B GPT-2 tokens from FineWebEdu dataset, the model matches GPT-3 124M model's HellaSwag evaluation score of 33.70%
                    </p>
                    <p><strong>Skills:</strong>Natural Language Processing (NLP), Large Language Models (LLM), Generative AI, Deep Learning</p>
                    <p>Click on the panels on the left to head over to the repository</p>
                </div>
            </div>

            <div class="project-item single-panel-project">
                <div class="project-image" onclick="window.open('https://github.com/JiteshChavan/U-Net-AutoEncoder-Variant-with-self-attention-block-in-bottleneck', '_blank')">
                    <img src="./images/unet.png" alt="U-Net AutoEncoder">
                </div>
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/nanogpt" target="_blank">
                        U-net based Neural Representation Learning Autoencoder Model</a></h3>
                    <p>
                        Implemented a U-Net based Neural Representation Learning Model from scratch in PyTorch with fully Convolutional encoder and Decoder with Self Attention Block to refine global context in the U-Net bottleneck.
                    </p>
                    <p><strong>Skills:</strong> Deep Learning, Computer Vision, Autoencoder Networks, Convolutional Neural Networks</p>
                    <p>The panel on the left illustrates reconstruction inferences using the trained U-Net.</p>
                </div>
            </div>

            <div class="project-item">
                <div class="project-info">
                    <h3><a href="https://github.com/JiteshChavan/Micrograd" target="_blank">MicrogradEngine</a></h3>
                    <p>
                        An atomic Autogradient engine. Implements backpropagation (reverse-mode autodifferentiation) 
                        over a dynamically built Directed Acyclic
                        Graph (DAG) andasmallneuralnetworkslibrary on top of it with a PyTorch-like API.
                    </p>
                    <p><strong>Skills:</strong> Pytorch, Python</p>
                </div>
            </div>
            <!-- Add more projects as needed -->
        </div>
    </section>

    <section id="publications">
        <h2>Publications</h2>

        
        <div class="projects-container">

            <div class="project-item">
                <div class="project-images">
                    <div class="project-image" onclick="window.open('https://arxiv.org/abs/2511.11243', '_blank')">
                        <img src="./images/arcee_main_results.png" alt="Project 2">
                    </div>
                    <div class="project-image" onclick="window.open('https://arxiv.org/abs/2511.11243', '_blank')">
                        <img src="./images/arcee_rc1qual.png" alt="Project 2 Additional Image">
                    </div>
                </div>
                <div class="project-info">
                    <h3><a href="https://arxiv.org/abs/2511.11243" target="_blank">Arcee: Differentiable Recurrent State Chain for Generative Vision Modeling with Mamba SSMs, first author, CVPR2026</a></h3>
                    <p>
                        Led design and implementation of Arcee, a structure-informed recurrent state chain for Mamba SSM vision backbones. Within a Flow Matching framework on CelebA-HQ 256×256, achieved up to 81% FID (82.81→15.33) and 88% KID (88.69→10.59) reductions over a naive Zigma baseline without adding parameters. Resulted in a first-author CVPR 2026 submission.
                    </p>
                </div>
            </div>
            
            <div class="project-item">
                <div class="project-info">
                    <h3>Lyapunov‑Based Stability Control for efficient sampling in Generative Flow Models,first author</a></h3>
                </div>
            </div>

            <div class="project-item">
                <div class="project-info">
                    <h3>Gradient Intervention for Mitigating Hallucinations in LLMs via Projection Layer Analysis,second author</a></h3>
                </div>
            </div>
            <!-- Add more projects as needed -->
        </div>
    </section>

    <section id="Articles">
        <h2>Articles</h2>
        <div class="experience-item">
            <p class="company"><a href="FlowMatching.html" target="_blank">Generative AI with ODEs and SDEs : An Introduction to Flow Matching and Diffusion Models</a></p> <!-- Company name moved here -->
            <h3>In progress</h3> <!-- Role moved here -->
            <p class="location-duration">Aug 2025</p>
        </div>
    </section>

    <section id="experience">
        <h2>Experience</h2>
        <div class="experience-item">
            <p class="company">Graduate Student Researcher</p> <!-- Company name moved here -->
            <h3>Ying Wu College of Computing (YWCC), NJIT </h3> <!-- Role moved here -->
            <p class="location-duration">Newark, NJ, USA | Feb 2024 - Dec 2025</p>
            <ul class="responsibilities">
                <li>Led design and implementation of Arcee, a structure-informed recurrent state chain for Mamba SSM vision backbones. Within a Flow Matching framework on CelebA-HQ 256×256, achieved up to 81% FID (82.81→15.33) and 88% KID (88.69→10.59) reductions over a naive Zigma baseline without adding parameters. Resulted in a first-author CVPR 2026 submission.</li>
                <li>Prototyped generative foundation models (Flow Matching, diffusion models, VAEs, GANs) for cross-modal tasks including text-/stroke-guided image generation, image inpainting, and protein structure synthesis, using PyTorch, Diffusers, and ControlNet-style conditioning.</li>
                <li>Implemented fused CUDA/C++ kernels to adapt Mamba selective-scan to non-sequential modalities, reducing training wall-clock and enabling efficient in-context learning.</li>
                <li>Built large-scale data and training pipelines for generative models and LLMs for symbolic math reasoning: streaming datasets (30M+ samples), FSDP-based multi-GPU training, automated FID/KID evaluation, and experiment tracking with Weights & Biases.</li>


            </ul>
        </div>
        <div class="experience-item">
            <p class="company">Software Engineer (Machine Learning)</p> <!-- Company name moved here -->
            <h3>Ford Motor Company</h3> <!-- Role moved here -->
            <p class="location-duration">Chennai, Tamil Nadu, India | July 2021 - Dec 2022</p>
            <ul class="responsibilities">
                <li>Designed and implemented a BERT-based NLP evaluation pipeline to automatically assess Ford’s recommendation models via semantic alignment against reference product and content data, replacing manual spot-checks with a scalable, metric-driven framework.</li>
                <li>Fine-tuned and productionized lightweight semantic transformer encoders in the recommendation stack, optimizing batching, caching, and model slimming to reduce end-to-end inference latency by 58% while maintaining recommendation quality.</li>
                <li>Collaborated with data scientists and product managers to define offline/online metrics (semantic similarity, CTR, engagement) and used A/B tests to show ML-driven personalization contributed to a 22% increase in transactions and 38% more personalized product interactions.</li>
                <li>Partnered with backend engineers to integrate ML services into Spring Boot/MVC microservices, exposing model inference APIs and feature-flagged rollouts for safe experimentation on high-traffic e-commerce flows.</li>
            </ul>
        </div>
        <div class="experience-item">
            <p class="company">Research Assistant, Computer Vision Lab</p> <!-- Company name moved here -->
            <h3>Visvesvaraya National Institute of Technology</h3> <!-- Role moved here -->
            <p class="location-duration">Nagpur, Maharashtra, India | July 2020 - May 2021</p>
            <ul class="responsibilities">
                <li>Initiated research on real‑time face detection & recognition using CNNs for single‑board computers, supervised by Prof. Kishor Bhurchandi. Resulting in 98% recall and 12‑21fps on 640x480 resolution video streams, achieving < 50MB post quantization memory footprint</li>
                <li>Implemented hardware‑aware optimizations for Haar Cascade Classifiers in C to boost performance on embedded systems.</li>
                <li>Worked on CNN based semantic segmentation models & researched adversarial training and mode collapse in GANs & its implications for synthetic data generation</li>
            </ul>
        </div>
    </section>

    <section id="skills">
        <h2>Skills</h2>
        <div class="skills-container">
            <!-- Skills will be dynamically populated here -->
        </div>
    </section>

    <section id="courses">
        <h2>Courses</h2>
        <div class="flair-line"></div> <!-- Flair line below the heading -->
        <div class="courses-container">
            <div class="course-card">
                An Introduction to Flow Matching and Diffusion Models | MIT 6.S184, IAP 2025
                <a href="https://diffusion.csail.mit.edu/" target="_blank" class="course-link">MIT OCW</a>
            </div>
            <div class="course-card">
                Introduction to Deep Learning | MIT 6.S191
                <a href="https://introtodeeplearning.com/" target="_blank" class="course-link">MIT OCW</a>
            </div>
            <div class="course-card">
                TinyML and Efficient Deep Learning Computing | MIT 6.5940
                <a href="https://hanlab.mit.edu/courses/2023-fall-65940" target="_blank" class="course-link">MIT OCW</a>
            </div>
            <div class="course-card">Computer Vision | ECL423
                <a href="https://vnit.ac.in/section/academics/wp-content/uploads/2023/07/B.Tech-in-Electronic-and-Communication-Enggineering.pdf" target="_blank" class="course-link">VNIT core coursework</a>
            </div>
            <div class="course-card">Multivariate Calculus, Integral Transforms and Partial Differential Equations | MAL201
                <a href="https://vnit.ac.in/section/academics/wp-content/uploads/2023/07/B.Tech-in-Electronic-and-Communication-Enggineering.pdf" target="_blank" class="course-link">VNIT core coursework</a>
            </div>
            <div class="course-card">Statistical Analysis and Queing Theory | MAL408
                <a href="https://vnit.ac.in/section/academics/wp-content/uploads/2023/07/B.Tech-in-Electronic-and-Communication-Enggineering.pdf" target="_blank" class="course-link">VNIT core coursework</a>
            </div>

            <div class="course-card">Machine Learning | CS675
                <a href="https://catalog.njit.edu/graduate/computing-sciences/computer-science/ms/" target="_blank" class="course-link">NJIT core coursework</a>
            </div>

            <div class="course-card">Artificial Intelligence | CS670
                <a href="https://catalog.njit.edu/graduate/computing-sciences/computer-science/ms/" target="_blank" class="course-link">NJIT core coursework</a>
            </div>

        </div>
    </section>
    <script src="script.js"></script>
</body>
</html>